# ai_service/token_manager.py

import json
import re
from typing import List, Dict, Any, Tuple

class TokenManager:
    """
    Token ç®¡ç†å™¨ï¼Œå¤„ç†ä¸åŒæ¨¡å‹çš„ token é™åˆ¶å’Œå†…å®¹ä¼˜åŒ–
    """
    
    # ä¸åŒæ¨¡å‹çš„ token é™åˆ¶
    MODEL_LIMITS = {
        'deepseek-v3': 32768,
        'deepseek-r1': 32768,
        'gpt-4': 8192,
        'gpt-4-32k': 32768,
        'gpt-4.1': 1047576,  
        'gpt-4.1-mini': 1047576, 
    }
    
    # ä¿ç•™ token æ•°é‡ï¼ˆç”¨äºå“åº”ç”Ÿæˆï¼‰
    RESPONSE_TOKENS = 800
    
    def __init__(self, model_name: str = 'deepseek-v3'):
        self.model_name = model_name
        self.max_tokens = self.MODEL_LIMITS.get(model_name, 32768)
        self.available_tokens = self.max_tokens - self.RESPONSE_TOKENS
        
        # ğŸš€ æ–°å¢ï¼šé’ˆå¯¹å¤§ä¸Šä¸‹æ–‡æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
        if self.max_tokens > 100000:  # å¯¹äºè¶…å¤§ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆå¦‚ GPT-4.1ï¼‰
            # ä¸ºè¶…å¤§æ¨¡å‹ä¿ç•™æ›´å¤šå“åº”ç©ºé—´
            self.RESPONSE_TOKENS = 2000
            self.available_tokens = self.max_tokens - self.RESPONSE_TOKENS
            print(f"Large context model detected: {model_name}, adjusted response tokens to {self.RESPONSE_TOKENS}")
        
    def estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡
        ç®€åŒ–ç®—æ³•ï¼šå¤§çº¦ 1 token = 4 å­—ç¬¦ï¼ˆå¯¹ä¸­æ–‡å’Œä»£ç éƒ½æœ‰ä¸€å®šå‡†ç¡®æ€§ï¼‰
        """
        if not text:
            return 0
        
        # åŸºç¡€å­—ç¬¦è®¡æ•°
        char_count = len(text)
        
        # å¯¹ä»£ç å†…å®¹è¿›è¡Œè°ƒæ•´ï¼ˆä»£ç é€šå¸¸ token å¯†åº¦æ›´é«˜ï¼‰
        if self._is_code_content(text):
            # ä»£ç çš„ token å¯†åº¦é€šå¸¸æ˜¯ 1 token â‰ˆ 3 å­—ç¬¦
            return int(char_count / 3)
        else:
            # æ™®é€šæ–‡æœ¬çš„ token å¯†åº¦æ˜¯ 1 token â‰ˆ 4 å­—ç¬¦
            return int(char_count / 4)
    
    def _is_code_content(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºä»£ç å†…å®¹
        """
        code_indicators = [
            '```', 'function', 'class ', 'def ', 'import ', 'require',
            '{}', '[]', '()', '=>', '->', '::',
            '+++', '---', '@@', 'diff'
        ]
        
        # å¦‚æœåŒ…å«å¤šä¸ªä»£ç æŒ‡ç¤ºç¬¦ï¼Œè®¤ä¸ºæ˜¯ä»£ç 
        indicator_count = sum(1 for indicator in code_indicators if indicator in text)
        return indicator_count >= 2
    
    def optimize_file_analysis_data(self, file_analysis_data: List[Dict]) -> List[Dict]:
        """
        ä¼˜åŒ–æ–‡ä»¶åˆ†ææ•°æ®ï¼Œç¡®ä¿æ€» token æ•°ä¸è¶…è¿‡é™åˆ¶
        æ³¨æ„ï¼šä¸é™åˆ¶æ–‡ä»¶æ•°é‡ï¼Œåªè¿›è¡Œå†…å®¹æˆªæ–­ä¼˜åŒ–ï¼ˆæ–‡ä»¶æ•°é‡é™åˆ¶ç”±æ’ä»¶è®¾ç½®æ§åˆ¶ï¼‰
        """
        if not file_analysis_data:
            return []
        
        # è®¡ç®—åŸºç¡€ prompt çš„ token æ•°ï¼ˆä¸åŒ…æ‹¬æ–‡ä»¶å†…å®¹ï¼‰
        base_prompt_tokens = self._estimate_base_prompt_tokens()
        available_for_files = self.available_tokens - base_prompt_tokens
        
        # å¦‚æœå¯ç”¨ token å¤ªå°‘ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if available_for_files < 100:
            return []
        
        optimized_files = []
        used_tokens = 0
        
        for file_data in file_analysis_data:
            # ä¼˜åŒ–å•ä¸ªæ–‡ä»¶çš„å†…å®¹
            optimized_file = self._optimize_single_file(file_data, available_for_files - used_tokens)
            
            if optimized_file:
                file_tokens = self._estimate_file_tokens(optimized_file)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¶³å¤Ÿçš„ç©ºé—´
                if used_tokens + file_tokens <= available_for_files:
                    optimized_files.append(optimized_file)
                    used_tokens += file_tokens
                else:
                    # å¦‚æœæ²¡æœ‰ç©ºé—´äº†ï¼Œå°è¯•è¿›ä¸€æ­¥å‹ç¼©
                    remaining_tokens = available_for_files - used_tokens
                    if remaining_tokens > 50:  # è‡³å°‘éœ€è¦ 50 tokens æ‰æœ‰æ„ä¹‰
                        compressed_file = self._compress_file_content(optimized_file, remaining_tokens)
                        if compressed_file:
                            optimized_files.append(compressed_file)
                    break
        
        return optimized_files
    
    def _estimate_base_prompt_tokens(self) -> int:
        """
        ä¼°ç®—åŸºç¡€ prompt çš„ token æ•°ï¼ˆä¸åŒ…æ‹¬æ–‡ä»¶å†…å®¹çš„éƒ¨åˆ†ï¼‰
        """
        # åŸºç¡€çš„ç³»ç»Ÿæ¶ˆæ¯å’Œæç¤ºç»“æ„å¤§çº¦éœ€è¦ 200-300 tokens
        return 300
    
    def _estimate_file_tokens(self, file_data: Dict) -> int:
        """
        ä¼°ç®—å•ä¸ªæ–‡ä»¶æ•°æ®çš„ token æ•°
        """
        tokens = 0
        
        # æ–‡ä»¶è·¯å¾„
        tokens += self.estimate_tokens(file_data.get('filePath', ''))
        
        # å˜æ›´ç±»å‹æè¿°
        tokens += 10
        
        # diff å†…å®¹
        diff_content = file_data.get('diffContent', '')
        tokens += self.estimate_tokens(diff_content)
        
        return tokens
    
    def _optimize_single_file(self, file_data: Dict, available_tokens: int) -> Dict:
        """
        ä¼˜åŒ–å•ä¸ªæ–‡ä»¶çš„å†…å®¹
        """
        if available_tokens < 50:  # å¤ªå°‘çš„ token æ— æ³•æœ‰æ•ˆåˆ†æ
            return None
        
        optimized_file = file_data.copy()
        diff_content = file_data.get('diffContent', '')
        
        if not diff_content:
            return optimized_file
        
        # è®¡ç®—å½“å‰æ–‡ä»¶çš„ token æ•°
        current_tokens = self._estimate_file_tokens(file_data)
        
        if current_tokens <= available_tokens:
            return optimized_file
        
        # éœ€è¦å‹ç¼© diff å†…å®¹
        target_diff_tokens = available_tokens - 30  # ä¸ºæ–‡ä»¶è·¯å¾„ç­‰å…¶ä»–ä¿¡æ¯é¢„ç•™ 30 tokens
        optimized_diff = self._compress_diff_content(diff_content, target_diff_tokens)
        optimized_file['diffContent'] = optimized_diff
        
        return optimized_file
    
    def _compress_file_content(self, file_data: Dict, max_tokens: int) -> Dict:
        """
        è¿›ä¸€æ­¥å‹ç¼©æ–‡ä»¶å†…å®¹ä»¥é€‚åº” token é™åˆ¶
        """
        if max_tokens < 30:
            return None
        
        compressed_file = file_data.copy()
        diff_content = file_data.get('diffContent', '')
        
        # æåº¦å‹ç¼©çš„ diff å†…å®¹
        target_diff_tokens = max_tokens - 20
        compressed_diff = self._compress_diff_content(diff_content, target_diff_tokens, aggressive=True)
        compressed_file['diffContent'] = compressed_diff
        
        return compressed_file
    
    def _compress_diff_content(self, diff_content: str, target_tokens: int, aggressive: bool = False) -> str:
        """
        å‹ç¼© diff å†…å®¹ä»¥é€‚åº” token é™åˆ¶
        """
        if not diff_content:
            return diff_content
        
        current_tokens = self.estimate_tokens(diff_content)
        
        if current_tokens <= target_tokens:
            return diff_content
        
        # è®¡ç®—å‹ç¼©æ¯”ä¾‹
        compression_ratio = target_tokens / current_tokens
        
        if aggressive:
            # æ¿€è¿›å‹ç¼©ï¼šåªä¿ç•™æœ€é‡è¦çš„å˜æ›´è¡Œ
            return self._aggressive_compress_diff(diff_content, target_tokens)
        else:
            # å¸¸è§„å‹ç¼©ï¼šä¿ç•™ç»“æ„ï¼Œæˆªæ–­å†…å®¹
            return self._regular_compress_diff(diff_content, compression_ratio)
    
    def _regular_compress_diff(self, diff_content: str, compression_ratio: float) -> str:
        """
        å¸¸è§„ diff å‹ç¼©ï¼šä¿ç•™é‡è¦ä¿¡æ¯ï¼Œæˆªæ–­å†—ä½™å†…å®¹
        """
        lines = diff_content.split('\n')
        
        # ä¿ç•™çš„è¡Œç±»å‹ä¼˜å…ˆçº§
        priority_patterns = [
            r'^@@.*@@',  # diff header
            r'^\+\+\+',  # file header
            r'^---',     # file header
            r'^\+.*',    # added lines
            r'^-.*',     # removed lines
        ]
        
        important_lines = []
        context_lines = []
        
        for line in lines:
            is_important = any(re.match(pattern, line) for pattern in priority_patterns)
            if is_important:
                important_lines.append(line)
            else:
                context_lines.append(line)
        
        # è®¡ç®—å¯ä»¥ä¿ç•™å¤šå°‘å†…å®¹
        total_chars = len('\n'.join(lines))
        target_chars = int(total_chars * compression_ratio)
        
        # å…ˆä¿ç•™é‡è¦è¡Œ
        result_lines = important_lines[:]
        current_chars = len('\n'.join(result_lines))
        
        # å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ éƒ¨åˆ†ä¸Šä¸‹æ–‡è¡Œ
        for line in context_lines:
            if current_chars + len(line) + 1 <= target_chars:
                result_lines.append(line)
                current_chars += len(line) + 1
            else:
                break
        
        result = '\n'.join(result_lines)
        
        # å¦‚æœå‹ç¼©åä»ç„¶å¤ªé•¿ï¼Œç›´æ¥æˆªæ–­
        if len(result) > target_chars:
            result = result[:target_chars] + '\n...[å†…å®¹å·²æˆªæ–­]'
        
        return result
    
    def _aggressive_compress_diff(self, diff_content: str, target_tokens: int) -> str:
        """
        æ¿€è¿› diff å‹ç¼©ï¼šåªä¿ç•™æœ€å…³é”®çš„å˜æ›´ä¿¡æ¯
        """
        lines = diff_content.split('\n')
        
        # æå–å…³é”®ä¿¡æ¯
        added_lines = [line for line in lines if line.startswith('+') and not line.startswith('+++')]
        removed_lines = [line for line in lines if line.startswith('-') and not line.startswith('---')]
        headers = [line for line in lines if line.startswith('@@')]
        
        # æ„å»ºç®€åŒ–çš„ diff
        summary_parts = []
        
        if headers:
            summary_parts.append(headers[0])  # ç¬¬ä¸€ä¸ª header
        
        # æ·»åŠ éƒ¨åˆ†æ–°å¢è¡Œ
        if added_lines:
            summary_parts.append(f"[æ–°å¢ {len(added_lines)} è¡Œ]")
            summary_parts.extend(added_lines[:3])  # åªä¿ç•™å‰3è¡Œ
            if len(added_lines) > 3:
                summary_parts.append(f"...[è¿˜æœ‰ {len(added_lines) - 3} è¡Œæ–°å¢]")
        
        # æ·»åŠ éƒ¨åˆ†åˆ é™¤è¡Œ
        if removed_lines:
            summary_parts.append(f"[åˆ é™¤ {len(removed_lines)} è¡Œ]")
            summary_parts.extend(removed_lines[:3])  # åªä¿ç•™å‰3è¡Œ
            if len(removed_lines) > 3:
                summary_parts.append(f"...[è¿˜æœ‰ {len(removed_lines) - 3} è¡Œåˆ é™¤]")
        
        result = '\n'.join(summary_parts)
        
        # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡ token æ•°
        target_chars = target_tokens * 4  # è¿‘ä¼¼è½¬æ¢
        if len(result) > target_chars:
            result = result[:target_chars] + '\n...[å·²æåº¦å‹ç¼©]'
        
        return result
    
    def validate_prompt_size(self, messages: List[Dict]) -> Tuple[bool, int, str]:
        """
        éªŒè¯ prompt å¤§å°æ˜¯å¦åœ¨æ¨¡å‹é™åˆ¶å†…
        
        Returns:
            (is_valid, estimated_tokens, recommendation)
        """
        total_tokens = 0
        
        for message in messages:
            content = message.get('content', '')
            total_tokens += self.estimate_tokens(content)
        
        is_valid = total_tokens <= self.available_tokens
        
        if not is_valid:
            excess_tokens = total_tokens - self.available_tokens
            recommendation = f"éœ€è¦å‡å°‘ {excess_tokens} tokensã€‚å»ºè®®å‡å°‘æ–‡ä»¶æ•°é‡æˆ–å‹ç¼©å†…å®¹ã€‚"
        else:
            remaining_tokens = self.available_tokens - total_tokens
            recommendation = f"token ä½¿ç”¨æ­£å¸¸ï¼Œè¿˜å‰©ä½™ {remaining_tokens} tokensã€‚"
        
        return is_valid, total_tokens, recommendation
    
    def get_optimization_stats(self, original_files: List[Dict], optimized_files: List[Dict]) -> Dict:
        """
        è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯
        """
        original_count = len(original_files)
        optimized_count = len(optimized_files)
        
        original_tokens = sum(self._estimate_file_tokens(f) for f in original_files)
        optimized_tokens = sum(self._estimate_file_tokens(f) for f in optimized_files)
        
        return {
            'original_file_count': original_count,
            'optimized_file_count': optimized_count,
            'files_removed': original_count - optimized_count,
            'original_tokens': original_tokens,
            'optimized_tokens': optimized_tokens,
            'tokens_saved': original_tokens - optimized_tokens,
            'compression_ratio': optimized_tokens / original_tokens if original_tokens > 0 else 0
        } 