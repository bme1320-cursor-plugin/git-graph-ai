# ai_service/prompt_config.py
from datetime import datetime
from token_manager import TokenManager

def get_file_change_type_description(type_char):
    """æ ¹æ®æ–‡ä»¶å˜æ›´ç±»å‹å­—ç¬¦è¿”å›ä¸­æ–‡æè¿°"""
    status_map = {
        'A': 'æ–°å¢',
        'M': 'ä¿®æ”¹',
        'D': 'åˆ é™¤',
        'R': 'é‡å‘½å',
        'U': 'æœªè·Ÿè¸ª'
    }
    return status_map.get(type_char, 'æœªçŸ¥')

def generate_stats_from_payload(file_changes, is_comparison=False):
    """ä»æ–‡ä»¶å˜æ›´æ•°æ®ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²"""
    stats = {'added': 0, 'modified': 0, 'deleted': 0, 'renamed': 0, 'untracked': 0}
    for f in file_changes:
        type_char = f.get('type')
        if type_char == 'A': stats['added'] += 1
        elif type_char == 'M': stats['modified'] += 1
        elif type_char == 'D': stats['deleted'] += 1
        elif type_char == 'R': stats['renamed'] += 1
        elif type_char == 'U': stats['untracked'] += 1
    
    parts = []
    if stats['added'] > 0: parts.append(f"{stats['added']}ä¸ªæ–°å¢æ–‡ä»¶")
    if stats['modified'] > 0: parts.append(f"{stats['modified']}ä¸ªä¿®æ”¹æ–‡ä»¶")
    if stats['deleted'] > 0: parts.append(f"{stats['deleted']}ä¸ªåˆ é™¤æ–‡ä»¶")
    if stats['renamed'] > 0: parts.append(f"{stats['renamed']}ä¸ªé‡å‘½åæ–‡ä»¶")

    if is_comparison:
        if stats['untracked'] > 0: parts.append(f"{stats['untracked']}ä¸ªæœªè·Ÿè¸ªæ–‡ä»¶")
        total_changes = sum(stats.values())
        return f"æœ¬æ¬¡æ¯”è¾ƒå…±æ¶‰åŠ {total_changes} ä¸ªæ–‡ä»¶å˜æ›´ï¼š{'ï¼Œ'.join(parts)}ã€‚"
    else:
        total_changes = stats['added'] + stats['modified'] + stats['deleted'] + stats['renamed']
        return f"æ­¤æäº¤å…±æ¶‰åŠ {total_changes} ä¸ªæ–‡ä»¶å˜æ›´ï¼š{'ï¼Œ'.join(parts)}ã€‚"

def build_analyze_diff_prompt(file_path, file_extension, file_name, file_diff):
    """æ„å»ºå•æ–‡ä»¶å·®å¼‚åˆ†æçš„æç¤º"""
    return f"""
            è¯·åˆ†æä»¥ä¸‹Gitå·®å¼‚æ–‡ä»¶çš„å˜æ›´å†…å®¹ï¼Œå¹¶æä¾›ç®€æ´çš„ä¸­æ–‡æ€»ç»“ã€‚

            æ–‡ä»¶ä¿¡æ¯ï¼š
            - æ–‡ä»¶è·¯å¾„: {file_path}
            - æ–‡ä»¶ç±»å‹: {file_extension}

            Git Diffå†…å®¹ï¼š
            ```diff
            {file_diff}
            ```

            è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼ˆä¸è¶…è¿‡80å­—ï¼‰ï¼š
            {file_name}: [æ–‡ä»¶ç®€ä»‹]ã€‚[ä¸»è¦å˜æ›´å†…å®¹]ã€‚

            è¦æ±‚ï¼š
            1. å…ˆç®€è¦è¯´æ˜æ–‡ä»¶ç”¨é€”
            2. é‡ç‚¹æè¿°å˜æ›´çš„åŠŸèƒ½æ€§å½±å“ï¼Œè€ŒéæŠ€æœ¯ç»†èŠ‚
            3. ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡è¡¨è¾¾
            4. é¿å…ä½¿ç”¨"è¿™ä¸ªæ–‡ä»¶"ç­‰æŒ‡ä»£è¯
            """

def build_comprehensive_commit_analysis_prompt(payload, model_name='deepseek-v3'):
    """æ„å»ºGitæäº¤çš„ç»¼åˆåˆ†ææç¤ºï¼Œæ”¯æŒ token ä¼˜åŒ–"""
    commit_details = payload.get('commitDetails', {})
    file_analysis_data = payload.get('fileAnalysisData', [])
    
    # ğŸš€ ä½¿ç”¨ token ç®¡ç†å™¨ä¼˜åŒ–æ–‡ä»¶æ•°æ®ï¼ˆåªè¿›è¡Œå†…å®¹æˆªæ–­ï¼Œä¸é™åˆ¶æ–‡ä»¶æ•°é‡ï¼‰
    token_manager = TokenManager(model_name)
    optimized_files = token_manager.optimize_file_analysis_data(file_analysis_data)
    
    # è®°å½•ä¼˜åŒ–ä¿¡æ¯
    if len(optimized_files) < len(file_analysis_data):
        print(f"ğŸ”§ Token optimization: {len(file_analysis_data)} -> {len(optimized_files)} files for {model_name}")
    
    stats = generate_stats_from_payload(optimized_files, is_comparison=False)

    prompt = f"""è¯·å¯¹ä»¥ä¸‹Gitæäº¤è¿›è¡Œç»¼åˆåˆ†æï¼Œæä¾›ä¸€ä¸ªæ•´ä½“æ€§çš„æ€»ç»“æŠ¥å‘Šã€‚

æäº¤ä¿¡æ¯ï¼š
- æäº¤å“ˆå¸Œ: {commit_details.get('hash', 'N/A')[:8]}...
- ä½œè€…: {commit_details.get('author', 'N/A')}
- æäº¤æ¶ˆæ¯: {_truncate_text(commit_details.get('body', 'æ— æäº¤æ¶ˆæ¯'), 100)}
- {stats}

ä¸»è¦æ–‡ä»¶å˜æ›´ï¼š
"""
    for index, file_data in enumerate(optimized_files):
        diff_content = file_data.get('diffContent', '')
        # ğŸš€ æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´diffå‹ç¼©çº§åˆ«
        if model_name in ['gpt-4.1', 'gpt-4.1-mini']:
            compressed_diff = _compress_diff_for_analysis(diff_content, max_lines=15)  # æ›´å¤šè¡Œæ•°
        else:
            compressed_diff = _compress_diff_for_analysis(diff_content, max_lines=8)   # æ ‡å‡†é™åˆ¶
        
        prompt += f"""
{index + 1}. æ–‡ä»¶: {file_data.get('filePath')}
   å˜æ›´ç±»å‹: {get_file_change_type_description(file_data.get('type'))}
   
   å·®å¼‚å†…å®¹:
   ```diff
   {compressed_diff}
   ```
"""
    
    # å¦‚æœæœ‰æ–‡ä»¶è¢«ä¼˜åŒ–å‹ç¼©ï¼Œæ·»åŠ è¯´æ˜
    if len(optimized_files) < len(file_analysis_data):
        prompt += f"\n[æ³¨ï¼šä¸ºæ§åˆ¶tokenä½¿ç”¨ï¼Œå·²ä¼˜åŒ– {len(file_analysis_data) - len(optimized_files)} ä¸ªæ–‡ä»¶çš„å†…å®¹]\n"

    prompt += """
è¯·æä¾›ä¸€ä¸ªç»¼åˆæ€§çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. è¿™æ¬¡æäº¤çš„ä¸»è¦ç›®çš„å’Œæ„å›¾
2. æ¶‰åŠçš„æ ¸å¿ƒåŠŸèƒ½æˆ–æ¨¡å—
3. å˜æ›´çš„æŠ€æœ¯å½±å“å’Œä¸šåŠ¡ä»·å€¼
4. ä»£ç è´¨é‡å’Œæ¶æ„æ–¹é¢çš„è§‚å¯Ÿ

è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- é‡ç‚¹å…³æ³¨æ•´ä½“æ€§å’Œå…³è”æ€§ï¼Œè€Œéå•ä¸ªæ–‡ä»¶çš„ç»†èŠ‚
- æ§åˆ¶åœ¨150å­—ä»¥å†…
- ä½¿ç”¨HTMLæ ¼å¼ï¼ŒåŒ…å«é€‚å½“çš„æ®µè½å’Œå¼ºè°ƒæ ‡ç­¾"""
    return prompt

def build_comprehensive_uncommitted_analysis_prompt(payload, model_name='deepseek-v3'):
    """æ„å»ºæœªæäº¤å˜æ›´çš„ç»¼åˆåˆ†ææç¤ºï¼Œæ”¯æŒ token ä¼˜åŒ–"""
    file_analysis_data = payload.get('fileAnalysisData', [])
    
    # ğŸš€ ä½¿ç”¨ token ç®¡ç†å™¨ä¼˜åŒ–æ–‡ä»¶æ•°æ®ï¼ˆåªè¿›è¡Œå†…å®¹æˆªæ–­ï¼Œä¸é™åˆ¶æ–‡ä»¶æ•°é‡ï¼‰
    token_manager = TokenManager(model_name)
    optimized_files = token_manager.optimize_file_analysis_data(file_analysis_data)
    
    stats = generate_stats_from_payload(optimized_files, is_comparison=True)

    prompt = f"""è¯·å¯¹ä»¥ä¸‹æœªæäº¤çš„ä»£ç å˜æ›´è¿›è¡Œç»¼åˆåˆ†æï¼Œæä¾›ä¸€ä¸ªæ•´ä½“æ€§çš„æ€»ç»“æŠ¥å‘Šã€‚

æœªæäº¤å˜æ›´ä¿¡æ¯ï¼š
- ç±»å‹: å·¥ä½œåŒºæœªæäº¤å˜æ›´
- {stats}

ä¸»è¦æ–‡ä»¶å˜æ›´ï¼š
"""
    for index, file_data in enumerate(optimized_files):
        diff_content = file_data.get('diffContent', '')
        # ğŸš€ æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´diffå‹ç¼©çº§åˆ«
        if model_name in ['gpt-4.1', 'gpt-4.1-mini']:
            compressed_diff = _compress_diff_for_analysis(diff_content, max_lines=12)  # æ›´å¤šè¡Œæ•°
        else:
            compressed_diff = _compress_diff_for_analysis(diff_content, max_lines=6)   # æ ‡å‡†é™åˆ¶
        
        prompt += f"""
{index + 1}. æ–‡ä»¶: {file_data.get('filePath')}
   å˜æ›´ç±»å‹: {get_file_change_type_description(file_data.get('type'))}
   
   å·®å¼‚å†…å®¹:
   ```diff
   {compressed_diff}
   ```
"""
    
    if len(optimized_files) < len(file_analysis_data):
        prompt += f"\n[æ³¨ï¼šä¸ºæ§åˆ¶tokenä½¿ç”¨ï¼Œå·²ä¼˜åŒ– {len(file_analysis_data) - len(optimized_files)} ä¸ªæ–‡ä»¶çš„å†…å®¹]\n"

    prompt += """
è¯·æä¾›ä¸€ä¸ªç»¼åˆæ€§çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. æœªæäº¤å˜æ›´çš„ä¸»è¦ç›®çš„å’Œæ„å›¾
2. æ¶‰åŠçš„æ ¸å¿ƒåŠŸèƒ½æˆ–æ¨¡å—
3. å˜æ›´çš„æŠ€æœ¯å½±å“å’Œä¸šåŠ¡ä»·å€¼
4. ä»£ç è´¨é‡å’Œæ¶æ„æ–¹é¢çš„è§‚å¯Ÿ
5. æäº¤å»ºè®®ï¼ˆæ˜¯å¦é€‚åˆæäº¤ã€éœ€è¦æ³¨æ„çš„äº‹é¡¹ç­‰ï¼‰

è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- é‡ç‚¹å…³æ³¨å˜æ›´çš„æ•´ä½“æ€§å’Œå…³è”æ€§ï¼Œè€Œéå•ä¸ªæ–‡ä»¶çš„ç»†èŠ‚
- æ§åˆ¶åœ¨150å­—ä»¥å†…
- ä½¿ç”¨HTMLæ ¼å¼ï¼ŒåŒ…å«é€‚å½“çš„æ®µè½å’Œå¼ºè°ƒæ ‡ç­¾"""
    return prompt

def build_comprehensive_comparison_prompt(payload, model_name='deepseek-v3'):
    """æ„å»ºç‰ˆæœ¬æ¯”è¾ƒçš„ç»¼åˆåˆ†ææç¤ºï¼Œæ”¯æŒ token ä¼˜åŒ–"""
    file_analysis_data = payload.get('fileAnalysisData', [])
    
    # ğŸš€ ä½¿ç”¨ token ç®¡ç†å™¨ä¼˜åŒ–æ–‡ä»¶æ•°æ®ï¼ˆåªè¿›è¡Œå†…å®¹æˆªæ–­ï¼Œä¸é™åˆ¶æ–‡ä»¶æ•°é‡ï¼‰
    token_manager = TokenManager(model_name)
    optimized_files = token_manager.optimize_file_analysis_data(file_analysis_data)
    
    stats = generate_stats_from_payload(optimized_files, is_comparison=True)

    prompt = f"""è¯·å¯¹ä»¥ä¸‹ç‰ˆæœ¬æ¯”è¾ƒè¿›è¡Œç»¼åˆåˆ†æï¼Œæä¾›ä¸€ä¸ªæ•´ä½“æ€§çš„æ€»ç»“æŠ¥å‘Šã€‚

æ¯”è¾ƒæ¦‚è§ˆï¼š
- {stats}

ä¸»è¦æ–‡ä»¶å˜æ›´ï¼š
"""
    for index, file_data in enumerate(optimized_files):
        diff_content = file_data.get('diffContent', '')
        # ğŸš€ æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´diffå‹ç¼©çº§åˆ«
        if model_name in ['gpt-4.1', 'gpt-4.1-mini']:
            compressed_diff = _compress_diff_for_analysis(diff_content, max_lines=10)  # æ›´å¤šè¡Œæ•°
        else:
            compressed_diff = _compress_diff_for_analysis(diff_content, max_lines=6)   # æ ‡å‡†é™åˆ¶
        
        prompt += f"""
{index + 1}. æ–‡ä»¶: {file_data.get('filePath')}
   å˜æ›´ç±»å‹: {get_file_change_type_description(file_data.get('type'))}
   
   å·®å¼‚å†…å®¹:
   ```diff
   {compressed_diff}
   ```
"""
    
    if len(optimized_files) < len(file_analysis_data):
        prompt += f"\n[æ³¨ï¼šä¸ºæ§åˆ¶tokenä½¿ç”¨ï¼Œå·²ä¼˜åŒ– {len(file_analysis_data) - len(optimized_files)} ä¸ªæ–‡ä»¶çš„å†…å®¹]\n"

    prompt += """
è¯·æä¾›ä¸€ä¸ªç»¼åˆæ€§çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. ä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„ä¸»è¦å·®å¼‚å’Œæ¼”è¿›æ–¹å‘
2. æ¶‰åŠçš„æ ¸å¿ƒåŠŸèƒ½å˜åŒ–
3. æ•´ä½“æ¶æ„æˆ–è®¾è®¡çš„æ”¹è¿›
4. æ½œåœ¨çš„å½±å“å’Œé£é™©è¯„ä¼°

è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- é‡ç‚¹å…³æ³¨ç‰ˆæœ¬é—´çš„æ•´ä½“å˜åŒ–è¶‹åŠ¿ï¼Œè€Œéå•ä¸ªæ–‡ä»¶çš„ç»†èŠ‚
- æ§åˆ¶åœ¨150å­—ä»¥å†…
- ä½¿ç”¨HTMLæ ¼å¼ï¼ŒåŒ…å«é€‚å½“çš„æ®µè½å’Œå¼ºè°ƒæ ‡ç­¾"""
    return prompt

def _compress_diff_for_analysis(diff_content, max_lines=8):
    """
    ä¸ºAIåˆ†æå‹ç¼©diffå†…å®¹
    """
    if not diff_content:
        return diff_content
    
    lines = diff_content.split('\n')
    
    # å¦‚æœè¡Œæ•°è¾ƒå°‘ï¼Œç›´æ¥è¿”å›
    if len(lines) <= max_lines:
        return diff_content
    
    # æå–é‡è¦è¡Œ
    important_lines = []
    context_lines = []
    
    for line in lines:
        if any(line.startswith(prefix) for prefix in ['+++', '---', '@@', '+', '-']):
            important_lines.append(line)
        else:
            context_lines.append(line)
    
    # ä¼˜å…ˆä¿ç•™é‡è¦è¡Œ
    result_lines = important_lines[:max_lines-1]
    
    # å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ ä¸€äº›ä¸Šä¸‹æ–‡
    remaining_space = max_lines - len(result_lines)
    if remaining_space > 0 and context_lines:
        result_lines.extend(context_lines[:remaining_space])
    
    # å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œæ·»åŠ è¯´æ˜
    if len(result_lines) < len(lines):
        result_lines.append(f"...[å·²å‹ç¼©ï¼ŒåŸå§‹å†…å®¹å…±{len(lines)}è¡Œ]")
    
    return '\n'.join(result_lines)

def _truncate_text(text, max_length=100):
    """
    æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šé•¿åº¦
    """
    if not text or len(text) <= max_length:
        return text
    
    return text[:max_length] + "..."

def build_file_history_analysis_prompt(payload):
    """æ„å»ºæ–‡ä»¶å†å²åˆ†æçš„æç¤º"""
    file_path = payload.get('filePath', 'æœªçŸ¥æ–‡ä»¶')
    commits = payload.get('commits', [])
    
    total_commits = len(commits)
    total_additions = sum(commit.get('additions', 0) or 0 for commit in commits)
    total_deletions = sum(commit.get('deletions', 0) or 0 for commit in commits)

    author_stats = {}
    for commit in commits:
        author = commit.get('author', 'æœªçŸ¥ä½œè€…')
        author_stats[author] = author_stats.get(author, 0) + 1
    
    top_authors = sorted(author_stats.items(), key=lambda item: item[1], reverse=True)[:3]
    top_authors_str = ', '.join([f"{author} ({count}æ¬¡æäº¤)" for author, count in top_authors])

    prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶çš„å†å²æ¼”è¿›æƒ…å†µï¼š

æ–‡ä»¶è·¯å¾„: {file_path}
æ€»æäº¤æ¬¡æ•°: {total_commits}
æ€»æ–°å¢è¡Œæ•°: {total_additions}
æ€»åˆ é™¤è¡Œæ•°: {total_deletions}
ä¸»è¦è´¡çŒ®è€…: {top_authors_str}

æœ€è¿‘çš„æäº¤å†å²ï¼š
"""
    for index, commit in enumerate(commits[:10]):
        date = datetime.fromtimestamp(commit.get('authorDate', 0)).strftime('%Y-%m-%d')
        change_type = get_file_change_type_description(commit.get('fileChange', {}).get('type'))
        message = commit.get('message', '').split('\n')[0][:100]
        additions = commit.get('additions', 0) or 0
        deletions = commit.get('deletions', 0) or 0
        prompt += f"""
{index + 1}. [{date}] {commit.get('author')}
   æäº¤: {message}
   å˜æ›´: {change_type} (+{additions}/-{deletions})
"""

    prompt += """
è¯·æä¾›ä¸€ä¸ªç»¼åˆæ€§çš„æ–‡ä»¶æ¼”è¿›åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡ä»¶æ¼”è¿›æ€»ç»“ï¼ˆæ•´ä½“å‘å±•è¶‹åŠ¿å’Œç›®çš„ï¼‰
2. æ¼”è¿›æ¨¡å¼ï¼ˆå¼€å‘æ´»è·ƒåº¦ã€å˜æ›´é¢‘ç‡ç­‰ï¼‰
3. å…³é”®å˜æ›´ç‚¹ï¼ˆé‡è¦çš„ä¿®æ”¹èŠ‚ç‚¹ï¼‰
4. ä¼˜åŒ–å»ºè®®ï¼ˆåŸºäºå†å²æ¨¡å¼çš„æ”¹è¿›å»ºè®®ï¼‰

è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡å›ç­”
- é‡ç‚¹å…³æ³¨æ–‡ä»¶çš„æ¼”è¿›è¶‹åŠ¿å’Œå¼€å‘æ¨¡å¼
- æ§åˆ¶åœ¨200å­—ä»¥å†…
- ä½¿ç”¨ç»“æ„åŒ–çš„JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«summaryã€evolutionPatternã€keyChangesã€recommendationså››ä¸ªå­—æ®µ"""
    return prompt

def build_enhanced_file_history_prompt(prompt):
    """ä¸ºæ–‡ä»¶å†å²åˆ†ææ„å»ºå¢å¼ºçš„æç¤º"""
    return f"""
è¯·å¯¹ä»¥ä¸‹æ–‡ä»¶çš„ç‰ˆæœ¬æ¼”è¿›å†å²è¿›è¡Œæ·±åº¦åˆ†æï¼š

{prompt}

è¦æ±‚ï¼šä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š

{{
    "summary": "è¯¥æ–‡ä»¶ä»å¼€å§‹åˆ°ç°åœ¨çš„æ•´ä½“æ¼”è¿›æ€»ç»“ï¼ŒåŒ…æ‹¬ä¸»è¦å‘å±•è¶‹åŠ¿å’Œç›®çš„",
    "evolutionPattern": "æ–‡ä»¶çš„æ¼”è¿›æ¨¡å¼åˆ†æï¼ŒåŒ…æ‹¬å¼€å‘æ´»è·ƒåº¦ã€å˜æ›´é¢‘ç‡ã€è´¡çŒ®è€…åä½œæ¨¡å¼ç­‰",
    "keyChanges": [
        "ç¬¬ä¸€ä¸ªé‡è¦çš„å…³é”®å˜æ›´ç‚¹",
        "ç¬¬äºŒä¸ªé‡è¦çš„å…³é”®å˜æ›´ç‚¹",
        "ç¬¬ä¸‰ä¸ªé‡è¦çš„å…³é”®å˜æ›´ç‚¹"
    ],
    "recommendations": [
        "ç¬¬ä¸€ä¸ªä¼˜åŒ–å»ºè®®",
        "ç¬¬äºŒä¸ªä¼˜åŒ–å»ºè®®",
        "ç¬¬ä¸‰ä¸ªä¼˜åŒ–å»ºè®®"
    ]
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å¿…é¡»ä¸¥æ ¼è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
2. keyChangeså’Œrecommendationså¿…é¡»æ˜¯å­—ç¬¦ä¸²æ•°ç»„ï¼Œæ¯é¡¹ä¸è¶…è¿‡35å­—
3. summaryå’ŒevolutionPatternä¸ºå­—ç¬¦ä¸²ï¼Œä¸è¶…è¿‡80å­—
4. ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ä¸“ä¸šä¸”æ˜“æ‡‚
5. ä¸è¦æ·»åŠ ```json```ä»£ç å—åŒ…è£…
6. åŸºäºå®é™…æäº¤å†å²æä¾›æœ‰ä»·å€¼çš„åˆ†æ
"""

def build_file_version_comparison_prompt(payload):
    """æ„å»ºæ–‡ä»¶ç‰ˆæœ¬æ¯”è¾ƒåˆ†æçš„æç¤º"""
    file_path = payload.get('filePath', 'æœªçŸ¥æ–‡ä»¶')
    from_hash = payload.get('fromHash', 'æœªçŸ¥ç‰ˆæœ¬')
    to_hash = payload.get('toHash', 'æœªçŸ¥ç‰ˆæœ¬')
    diff_content = payload.get('diffContent', '')
    content_before = payload.get('contentBefore', '')
    content_after = payload.get('contentAfter', '')
    
    file_name = file_path.split('/')[-1] if '/' in file_path else file_path
    file_extension = file_name.split('.')[-1].lower() if '.' in file_name else ''

    prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡ä»¶ç‰ˆæœ¬æ¯”è¾ƒè¿›è¡Œæ·±åº¦åˆ†æï¼š

æ–‡ä»¶ä¿¡æ¯ï¼š
- æ–‡ä»¶è·¯å¾„ï¼š{file_path}
- æ–‡ä»¶åï¼š{file_name}
- æ–‡ä»¶ç±»å‹ï¼š{file_extension}
- æºç‰ˆæœ¬ï¼š{from_hash[:8]}
- ç›®æ ‡ç‰ˆæœ¬ï¼š{to_hash[:8]}

Git Diffå†…å®¹ï¼š
```diff
{diff_content}
```
"""
    if content_before and content_after:
        prompt += f"""
ç‰ˆæœ¬å‰å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰ï¼š
```
{content_before[:200]}{'...' if len(content_before) > 200 else ''}
```

ç‰ˆæœ¬åå†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰ï¼š
```
{content_after[:200]}{'...' if len(content_after) > 200 else ''}
```
"""
    prompt += """
è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼æä¾›åˆ†æç»“æœï¼š

{
  "summary": "è¿™æ¬¡æ–‡ä»¶å˜æ›´çš„ç®€è¦æ€»ç»“ï¼ˆä¸è¶…è¿‡100å­—ï¼‰",
  "changeType": "å˜æ›´ç±»å‹æè¿°ï¼ˆå¦‚ï¼šåŠŸèƒ½å¢å¼ºã€bugä¿®å¤ã€é‡æ„ç­‰ï¼‰",
  "impactAnalysis": "å˜æ›´å½±å“åˆ†æï¼ˆå¯¹ç³»ç»Ÿã€ç”¨æˆ·ã€æ€§èƒ½ç­‰æ–¹é¢çš„å½±å“ï¼‰",
  "keyModifications": [
    "ç¬¬ä¸€ä¸ªå…³é”®ä¿®æ”¹ç‚¹",
    "ç¬¬äºŒä¸ªå…³é”®ä¿®æ”¹ç‚¹",
    "ç¬¬ä¸‰ä¸ªå…³é”®ä¿®æ”¹ç‚¹"
  ],
  "recommendations": [
    "ç¬¬ä¸€ä¸ªå»ºè®®æˆ–æ³¨æ„äº‹é¡¹",
    "ç¬¬äºŒä¸ªå»ºè®®æˆ–æ³¨æ„äº‹é¡¹"
  ]
}

è¦æ±‚ï¼š
1. ä¸¥æ ¼è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹
2. æ‰€æœ‰å­—æ®µéƒ½ç”¨ä¸­æ–‡å¡«å†™
3. keyModificationså’Œrecommendationsæ•°ç»„æ¯é¡¹ä¸è¶…è¿‡50å­—
4. åˆ†æè¦ä¸“ä¸šä¸”æœ‰ä»·å€¼
5. åŸºäºå®é™…çš„ä»£ç å˜æ›´æä¾›è§è§£"""
    return prompt 